<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Monte Carlo Markov Chains - James Alvey</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="James Alvey" property="og:site_name">
  
    <meta content="Monte Carlo Markov Chains" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of *sampling*." property="og:description">
  
  
    <meta content="http://localhost:4000/mcmc/" property="og:url">
  
  
    <meta content="2019-08-20T01:00:00+01:00" property="article:published_time">
    <meta content="http://localhost:4000/about/" property="article:author">
  
  
    <meta content="http://localhost:4000/assets/img/monte-carlo.jpg" property="og:image">
  
  
    
  
  
    
    <meta content="Monte Carlo Markov Chains" property="article:tag">
    
    <meta content="MCMC" property="article:tag">
    
    <meta content="Python" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@James_A_42# add your Twitter handle">
  
    <meta name="twitter:title" content="Monte Carlo Markov Chains">
  
  
    <meta name="twitter:url" content="http://localhost:4000/mcmc/">
  
  
    <meta name="twitter:description" content="Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of *sampling*.">
  
  
    <meta name="twitter:image:src" content="http://localhost:4000/assets/img/monte-carlo.jpg">
  

	<meta name="description" content="Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of *sampling*.">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body">

  <div class="wrapper">
    <aside class="sidebar">
  <header style="overflow-y: scroll;">
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/james-alvey.jpg" alt="James Alvey"></a>
      </div>
      <div class="author-name">James Alvey</div>
      <p class="about-description" align="justify">I am a PhD student in Theoretical Particle Physics and Comsology at King's College London. I am interested in how to model the early Universe and use data from Cosmology to constrain new physics.</p>
    <section class="contact">
      <ul>
        
          <li class="github"><a href="http://github.com/james-alvey-42" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="email"><a href="mailto:AlveyJBG@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/james-alvey-a75845102" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li><a href="https://twitter.com/James_A_42# add your Twitter handle" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    </div>
    <section class="links">
      <h3 class="links-title">Links</h3>
      <ul>
          <li>Inspire-HEP Profile (Publications)</br>
            <a href="http://inspirehep.net/author/profile/J.B.G.Alvey.1" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i>inspirehep.net</a>
          </li></br>
          <li>Career (CV) </br>
            <a href="/assets/pdf/james-alvey-cv.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>PDF</a>
          </li></br>
      </ul>
    </section>
    <p></p>
    <section class="links">
      <h3 class="links-title">Articles</h3>
      <ul>
          <li><a href="/bbn" style="color: #ffffff">DarkBBN</a> <a href="http://github.com/james-alvey-42/DarkBBN" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1910.01649.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1910.01649</a>
            <a href="https://arxiv.org/pdf/1910.10730.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1910.10730</a>
            <a href="/assets/pdf/DMUK.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>DMUK Presentation</a>
          </li></br>
          <li><a href="/boosteddm" style="color: #ffffff">Inelastic Cosmic Rays</a> <a href="http://github.com/james-alvey-42/BoostedDM" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1905.05776.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1905.05776</a>
            <a href="/assets/pdf/beam-dump.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>BUSSTEPP Presentation</a>
          </li></br>
          <li><a href="/neutrinos" style="color: #ffffff">Neutrino Astrophysics</a> <a href="http://github.com/james-alvey-42/IceCubeNeutrinos" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1902.01450.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1902.01450</a>
          </li></br>
      </ul>
    </section>
    <p></p>
    <section class="links">
      <h3 class="links-title">More</h3>
      <ul>
          <li><a href="/pythonperformance" style="color: #ffffff">Free Speed in Python</a> <a href="http://github.com/james-alvey-42/ProgramTools/tree/master/PythonLessons" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="/assets/pdf/free-speed.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>Presentation</a>
          </li></br>
          <li><a href="/mcmc" style="color: #ffffff">Monte Carlo Markov Chains</a> <a href="http://github.com/james-alvey-42/ProgramTools/tree/master/MCMC" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a>
          </li></br>
          <li>Science Communication
            <a href="https://www.varsity.co.uk/profile/james-alvey" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i>varsity.co.uk</a>
          </li></br>
      </ul>
    </section>
  </header> <!-- End Header -->
  <footer>
    <div class="copyright">
      <p>2019 &copy; James Alvey</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/monte-carlo.jpg alt="Monte Carlo Markov Chains">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Monte Carlo Markov Chains</h1>
        <div class="page-date"><span>2019, Aug 20&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <p>Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of <em>sampling</em>.</p>

<h2 id="the-sampling-problem">The Sampling Problem</h2>

<p>We can state the sampling problem as follows;</p>

<blockquote>
  <p>Let <script type="math/tex">\mathcal{D}</script> be a distribution over a finite set <script type="math/tex">X</script>. Further, we assume that we have access to <script type="math/tex">p(x)</script> for some <script type="math/tex">x in X</script> which outputs the probabiliity of drawing <script type="math/tex">x</script> given <script type="math/tex">\mathcal{D}</script>. The sampling problem is to design an algorithm <script type="math/tex">\mathcal{A}</script> which outputs an element of <script type="math/tex">x</script> approximately with probability <script type="math/tex">p(x)</script>.</p>
</blockquote>

<p>The reason this is a problem is the following: suppose you generate a large number of choices <script type="math/tex">\{x_0, x_1, x_2, \cdots\}</script> from the underlying set according to some distribtuion that is <em>not</em> <script type="math/tex">p(x)</script>. As it stands, this is certainly not a sample that is close to being representative of the underlying distribution. To generate a genuine sample from this selection, we have to apply some criterion to each point. Note that we have access to <script type="math/tex">\{p(x_0), p(x_1), \cdots\}</script>, so this criterion might take the form of the following:</p>

<ol>
  <li>Simulate the list <script type="math/tex">\{x_0, x_1, \cdots\}</script></li>
  <li>For each data point <script type="math/tex">x_k</script>, compute <script type="math/tex">p(x_k)</script></li>
  <li>Generate a random number <script type="math/tex">p \sim U[0, 1]</script></li>
  <li>If <script type="math/tex">% <![CDATA[
p < p(x_k) %]]></script>, add the point <script type="math/tex">x_k</script> to the sample, else reject the point</li>
</ol>

<p>The probelm with this is that for a large set <script type="math/tex">X</script>, there might only be a small region where the probability is non-neglible, so the vast majority of the points in the original list will be rejected. The above approach will generate a sample that reflects the underlying distribution, the problem is that it will be very slow. To show that this really does work, consider the following example.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">N</span>         <span class="o">=</span> <span class="mf">1e6</span>
<span class="n">x</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">p</span>         <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">p_test</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_test</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">)</span>
<span class="n">sample</span>    <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">criterion</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'$p(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">sample</span><span class="p">],</span> 
	<span class="n">bins</span><span class="o">=</span><span class="mf">1e2</span><span class="p">,</span> 
	<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
	<span class="n">histtype</span><span class="o">=</span><span class="s">'step'</span><span class="p">,)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>This produces the graphical output below. Testing the code, we find that approximately 90% of the original sample is rejected, illustrating the issue with the method to generate samples of a given size.</p>

<p><img src="/assets/img/easy_sample.png" alt="Easy Sample" /></p>

<blockquote>
  <p>Using the easy sampling method, the probability criterion is applied to the blue sample, resulting in the correct green sample for a normal distribution with mean 0, and variance 1.*</p>
</blockquote>

<h2 id="the-solution-markov-chains">The Solution: Markov Chains</h2>

<p>To solve this sampling problem, we turn to the second MC in the title, <em>Markov Chains</em>. We can view a Markoc Chain as a random walk on a graph in that sense that for a given graph of vertices and edges <script type="math/tex">G = (V, E)</script>, we should specify a number <script type="math/tex">p_{uv} \in [0, 1]</script> for each edge <script type="math/tex">e = (u, v) \in E</script>. For this to be a true random walk, the probability should satisfy the condition that for every vertex <script type="math/tex">v \in V</script>, <script type="math/tex">\sum_y{p_{xy}} = 1</script> where the sum is over all outgoing edges. In other words, the outgoing values from the vertex form a probability distribution. To proceed, we need to appeal to the <em>Fundamental Theorem of Markov Chains</em> which states the following:</p>

<h3 id="fundamental-theorem-of-markov-chains">Fundamental Theorem of Markov Chains</h3>

<blockquote>
  <p>For any irreducible, aperiodic, positive-recurrent Markov Chain there exists a <em>unique stationary distribution</em> <script type="math/tex">\pi_j, j \in \mathbb{Z}</script>. Intuitively this says that the probability you end up on a given vertex is independent of where you start, and that the given distribution is uniquely determined by the Markov Chain.</p>
</blockquote>

<p>Now, to introduce the stationary distribution, we will represent the transition probabilities between states <script type="math/tex">i</script> and <script type="math/tex">j</script> as entries in a matrix <script type="math/tex">A = (A_{ij})</script>. The stationary distribution <script type="math/tex">\pi</script> then satisfies <script type="math/tex">A \pi = \pi</script> i.e. it is an <em>eigenvector</em> of <script type="math/tex">A</script> with eigenvalue 1. To guarantee the existence of a unique such vector, there are necessarily conditions on the matrix, or equivalently on the Markov Chains, but these will be satisfied by construction in the case at hand.</p>

<h3 id="constructing-a-graph-to-wak-on">Constructing a Graph to Wak On</h3>

<p>We are now in a position to understand the MCMC method;</p>

<blockquote>
  <p>The MCMC method is as follows: we want to sample over a finite set <script type="math/tex">X</script> with probability function <script type="math/tex">p(x)</script>. To do so, we construct a Markov Chain whose stationary distribution is exactly <script type="math/tex">p</script>. This is equivalent to choosing a graph and a set of transition probabilities. The sample is then generated by performing a random walk on the graph and listing the vertices. In the long term, the time spent at each vertex will be proportional to the stationary distribution.</p>
</blockquote>

<p>Now we can construct the probability distribution on a given lattice <script type="math/tex">\{0, 1, \cdots, n\}^d</script>. Then, let <script type="math/tex">r = 2d</script> and suppose we are at some vertex <script type="math/tex">i</script>. To choose where to go next we do the following;</p>

<ol>
  <li>Pick a neighbouring vertex <script type="math/tex">j</script> with probability <script type="math/tex">1/r</script>, else stay at <script type="math/tex">i</script></li>
  <li>If you pick <script type="math/tex">j</script> <em>and</em> <script type="math/tex">p(j) \geq p(i)</script>, go to <script type="math/tex">j</script> deterministically</li>
  <li>Otherwise go to <script type="math/tex">j</script> with probability <script type="math/tex">p(j)/p(i)</script>.</li>
</ol>

<p>To prove that this really is stationary, we note that we can write,</p>

<script type="math/tex; mode=display">p_{i, j} = \frac{1}{r} \mathrm{min}\left(1, p(j)/p(i)\right), \quad p_{i, i}=1-\sum_{(i, j) \in E(G) ; j \neq i} p_{i, j}</script>

<p>Then use the fact that if a probability distribution, <script type="math/tex">\pi(x)</script> is stationary, then <script type="math/tex">\pi(x)p_{x, y} = \pi(y)p_{y, x}</script> (this is the statement of detailed balance). Summing over the right hand side this gives exactly <script type="math/tex">\pi(x) = \pi(y)p_{y, x}</script> where we sum over <script type="math/tex">y</script> implicitly. This is indeed the eigenvalue equation. Doing this with our choice of probability distribution, we can work only with the first expression and note that,</p>

<script type="math/tex; mode=display">p(i)p_{i, j} = p(j)p_{j, i} = \frac{1}{r}\mathrm{min}\left(p(i), p(j)\right)</script>

<p>So, we have found a graph that has a suitable stationary distribution. To generate a sample, we simply randomly choose a starting point (maybe after jumping round the graph a bit) and compute a random walk on the graph according to these probabilities.</p>

<h2 id="an-example-on-mathbbz">An Example on <script type="math/tex">\mathbb{Z}</script></h2>

<p>Consider the following probability distribution defined on the integers,</p>

<script type="math/tex; mode=display">% <![CDATA[
p(k) = \begin{cases} \frac{3}{\pi^2 + 3} & k = 0 \\  \frac{3}{\pi^2 + 3}\frac{1}{k^2} & k \neq 0 \end{cases} %]]></script>

<p>This is normalised thanks to the result that <script type="math/tex">\sum_{k = 0}^{\infty}{k^{-2}} = \pi^2/6</script>. We define this as well as the relevant Metropolis-Hastings algorithm in the following functions,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
	<span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>
	<span class="k">else</span><span class="p">:</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">metropolis</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
	<span class="c1"># Choose a random integer starting at zero
</span>	<span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="n">sample</span> <span class="o">=</span> <span class="p">[]</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
		<span class="c1"># Choose neighbour with probability 1/2
</span>		<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
			<span class="n">neighbour</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">neighbour</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span>
		<span class="c1"># Go to neighbour deterministically if p(neighbour) &gt; p(k)
</span>		<span class="k">if</span> <span class="n">p</span><span class="p">(</span><span class="n">neighbour</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
			<span class="n">k</span> <span class="o">=</span> <span class="n">neighbour</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="c1"># Go to neighbour with probability p(neighbour)/p(k)
</span>			<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">(</span><span class="n">neighbour</span><span class="p">)</span><span class="o">/</span><span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
				<span class="n">k</span> <span class="o">=</span> <span class="n">neighbour</span>
		<span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span></code></pre></figure>

<p>We are now in a position to test our algorithm. We find the results shown below for <script type="math/tex">N =</script> and <script type="math/tex">N =</script>. There are a couple of observations to make. Firstly, we see that as we increase <script type="math/tex">N</script>, the random walk explores more of the graph and represents the stationary distribution better. On the other hand, due to the fact that in order to explore the extremities, there is a small probability of going back. Hence we see that the results tend to be biased towards one direction.</p>

<p><img src="/assets/img/smallN.png" alt="smallN" /> <img src="/assets/img/largeN.png" alt="largeN" /></p>

<p>To counter this latter problem, we can instead take multiple samples using the Metropolis algorithm and combine them to form one large sample. This will ensure that although each sample is likely to be biased towards positive or negative values, the total sample will not be. This has the additional benefit of being easily parallelisable using Python’s <a href="https://joblib.readthedocs.io/en/latest/">joblib library</a> which is implemented as follows,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="n">N</span><span class="p">,</span> <span class="n">Ns</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">1e2</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">metropolis</span><span class="p">)(</span><span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">Ns</span><span class="p">))))</span></code></pre></figure>

<p>This gives much more symmetrical results that nonetheless explore much of the feature space.</p>

<p><img src="/assets/img/betterN.png" alt="betterN" /></p>

<h2 id="another-example-the-double-gaussian">Another Example: The Double Gaussian</h2>

<p>To further illustrate the issue of not exploring the full parameter space, we consider the example of a continuous random variable whose p.d.f. is defined by,</p>

<script type="math/tex; mode=display">f(x; \mu, \sigma^2) = \frac{1}{2\sqrt{2\pi}\sigma} \left[\exp\left(\frac{(x - \mu)^2}{2\sigma^2}\right) + \exp\left(\frac{(x + \mu)^2}{2\sigma^2}\right)\right]</script>

<p>which is essentially a sum of two Gaussians centred at <script type="math/tex">\pm \mu</script>. We now run the continuous equivalent of the Metropolis-Hastings algorithm for this double Gaussian with <script type="math/tex">\mu = 0</script> and <script type="math/tex">\mu = 2.0</script>. This is shown in the figure below, where we see that indeed in the latter case, the algorithm gets stuck on one side of the distribution, unable to jump to the other maxima.</p>

<p><img src="/assets/img/MCMC_bad.png" alt="bad" /> <img src="/assets/img/MCMCjoint_bad.png" alt="badJoint" /></p>

<p>Taking the same approach as in the discrete case, we now parallelise the sampling and concatenate the result. This leads to much better performance as shown below.</p>

<p><img src="/assets/img/MCMC_good.png" alt="good" /> <img src="/assets/img/MCMCjoint_good.png" alt="goodJoint" /></p>

<p>The full code for this example is given below:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gaussian_kde</span>

<span class="k">class</span> <span class="nc">DoubleGaussian</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
		<span class="k">def</span> <span class="nf">normal_dist</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
			<span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> \
				 <span class="o">+</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">mu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span> <span class="o">=</span> <span class="n">normal_dist</span>

<span class="k">def</span> <span class="nf">metropolis</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
	<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
	<span class="n">p</span> <span class="o">=</span> <span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">pts</span> <span class="o">=</span> <span class="p">[]</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
		<span class="n">xn</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">pn</span> <span class="o">=</span> <span class="n">dist</span><span class="p">(</span><span class="n">xn</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">pn</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="p">:</span>
			<span class="n">p</span> <span class="o">=</span> <span class="n">pn</span>
			<span class="n">x</span> <span class="o">=</span> <span class="n">xn</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span>
			<span class="k">if</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">pn</span><span class="o">/</span><span class="n">p</span><span class="p">:</span>
				<span class="n">p</span> <span class="o">=</span> <span class="n">pn</span>
				<span class="n">x</span> <span class="o">=</span> <span class="n">xn</span>
		<span class="n">pts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
	<span class="n">pts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">pts</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>

    <span class="n">N</span> <span class="o">=</span> <span class="mi">500</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="n">pdf</span> <span class="o">=</span> <span class="n">DoubleGaussian</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Mean:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Sigma:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'p(0):</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="s">'{:.3f}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)))</span>

    <span class="n">pts1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">metropolis</span><span class="p">)(</span><span class="n">dist</span><span class="o">=</span><span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> \
    										  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)))</span>

    <span class="c1"># kernel = gaussian_kde(pts1)
</span>    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="c1"># plt.plot(test, kernel.evaluate(test), 
</span>    <span class="c1"># 	ls='--',
</span>    <span class="c1"># 	c='k',
</span>    <span class="c1"># 	lw=1.0)
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">),</span>
    	<span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span>
    	<span class="n">c</span><span class="o">=</span><span class="s">'#D1495B'</span><span class="p">,</span>
    	<span class="n">lw</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">r'$\mathrm{True}\,\,\mathrm{Distr.}$'</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span> 
    	<span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
    	<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    	<span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    	<span class="n">label</span><span class="o">=</span><span class="s">r'$\mu = {:.1f}, \sigma = {:.1f}$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span>
    	<span class="n">histtype</span><span class="o">=</span><span class="s">'step'</span>
    	<span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="mf">0.5</span>

    <span class="n">pdf</span> <span class="o">=</span> <span class="n">DoubleGaussian</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Mean:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">mu</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'Sigma:</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">sigma</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'p(0):</span><span class="se">\t</span><span class="s">'</span><span class="p">,</span> <span class="s">'{:.3f}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)))</span>

    <span class="n">pts2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">metropolis</span><span class="p">)(</span><span class="n">dist</span><span class="o">=</span><span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> \
    										  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)))</span>

    <span class="c1"># kernel = gaussian_kde(pts2)
</span>    <span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="c1"># plt.plot(test, kernel.evaluate(test), 
</span>    <span class="c1"># 	ls='--',
</span>    <span class="c1"># 	c='k',
</span>    <span class="c1"># 	lw=1.0)
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">pdf</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test</span><span class="p">),</span>
    	<span class="n">ls</span><span class="o">=</span><span class="s">'--'</span><span class="p">,</span>
    	<span class="n">c</span><span class="o">=</span><span class="s">'#D1495B'</span><span class="p">,</span>
    	<span class="n">lw</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">pts2</span><span class="p">,</span> 
    	<span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> 
    	<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
    	<span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> 
    	<span class="n">label</span><span class="o">=</span><span class="s">r'$\mu = {:.1f}, \sigma = {:.1f}$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span>
    	<span class="n">histtype</span><span class="o">=</span><span class="s">'step'</span>
    	<span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'$p(x)$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'$N = {}$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">title_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="c1">#plt.title(r'MCMC: Double Gaussian')
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'MCMC_good.png'</span><span class="p">)</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">jointplot</span><span class="p">(</span><span class="n">pts1</span><span class="p">,</span> <span class="n">pts2</span><span class="p">,</span>
    	<span class="n">kind</span><span class="o">=</span><span class="s">'hex'</span><span class="p">,</span>
    	<span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span>
    	<span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span>
    	<span class="n">color</span><span class="o">=</span><span class="s">'#D1495B'</span>

    	<span class="p">)</span><span class="o">.</span><span class="n">set_axis_labels</span><span class="p">(</span><span class="s">r'$\mathcal{N}_2(0, 1)$'</span><span class="p">,</span> <span class="s">r'$\mathcal{N}_2(2, 0.5)$'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'MCMCjoint_good.png'</span><span class="p">)</span></code></pre></figure>

<h2 id="bayesian-parameter-estimation">Bayesian Parameter Estimation</h2>

<p>THe other major use of Monte Carlo Markov Chains is in Bayesian parameter estimation. A prominent example of this is in Cosmology, where <script type="math/tex">\Lambda\mathrm{CDM}</script> models are fitted to CMB/Galaxy survey/weak lensing data. In this case, instead of sampling from a distribution, the aim is to estimate the posterior distribution (i.e. generate a representative sample that approximately represents the underlying posterior). This process is encoded in Bayes’ rule; suppose we have some data <script type="math/tex">X = \{X_0, X_1, X_2, \cdots\}</script> and a statistical model that tells us the probabiltiy of a given realisation of the data given some set of parameters <script type="math/tex">\theta = (\theta_1, \theta_2, \cdots)</script>. Suppose further that we also have a <em>prior</em> view on the value of the parameters, denoted <script type="math/tex">p(\theta)</script>. Bayes’ theorem then lets us calculate the probability that the set of parameters are the true parameters given the data, the <em>posterior</em> distribution;</p>

<script type="math/tex; mode=display">p(\theta | X) = \frac{p(X | \theta)p(\theta)}{P(X)}, \quad p(X) = \int{\mathrm{d}\mu_\theta p(X | \theta) p(\theta)}</script>

<p>Now, the reason MCMC methods are useful in this case is that the denominator of this fraction is difficult to calculate in general especially for complex models. Since the MCMC only requires ratios of posterior probabilities however, this difficult factor cancels out in expressions such as,</p>

<script type="math/tex; mode=display">\frac{p(\theta_i | X)}{p(\theta_j | X)} = \frac{p(X | \theta_i) p(\theta_i)}{p(X | \theta_j) p(\theta_j)}</script>

<p>which is essentially a likelihood ratio. Now we can simply follow the Metropolis-Hastings algorithm to use a Monte Carlo Markov Chain to sample from the posterior distribution. This has the benefit that we can estimate not only the best-fit parameters, but also an error in these parameters since we have access to the full distribution. We can implement the algorithm as follows,</p>

<ol>
  <li>Sample a random point in the parameter space <script type="math/tex">\theta_i</script> according to the prior</li>
  <li>Sample another random point in the parameter space <script type="math/tex">\theta_j</script></li>
  <li>Compute <script type="math/tex">\ell(X, \theta_i) = p(X; \theta_i)p(\theta_i)</script> and <script type="math/tex">\ell(X, \theta_j) = p(X; \theta_j)p(\theta_j)</script></li>
  <li>If <script type="math/tex">\ell(X, \theta_j) > \ell(X, \theta_i)</script>, keep the new value, <script type="math/tex">\theta_j</script>, then repeat, else keep the old value with probabiltiy <script type="math/tex">1 - \ell(X, \theta_j)/\ell(X, \theta_i)</script></li>
</ol>

<p>This final step can be rewritten in terms of the log-likelihood. If we accept the new parameter values with probability <script type="math/tex">p \sim U[0, 1]</script>, then we accept the update if,</p>

<script type="math/tex; mode=display">% <![CDATA[
\log p < \sum_{i \in \mathrm{data}}{\left[\log p(x_i | \theta_j) - \log p(x_i | \theta_i)\right]} + \log p(\theta_j) - \log p(\theta_i) %]]></script>

<h2 id="an-example-fitting-a-normal-distribution">An Example: Fitting a Normal Distribution</h2>

<p>We take possibly the simplest example and consider data distributed according to a standard normal distribution, <script type="math/tex">X \sim \mathcal{N}(0, 1)</script>. Then we want to find the posterior distribution on the mean, <script type="math/tex">\mu</script>, for fixed variance. The code to implement this is shown below in the case of a flat prior,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">class</span> <span class="nc">BayesModel</span><span class="p">:</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">prior_range</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
		<span class="k">def</span> <span class="nf">pdata</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">):</span>
			<span class="s">r"""
			Probability of :math:`x` given :math:`theta`, assuming :math:`\sigma = 1`.
			"""</span>
			<span class="k">return</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">)</span><span class="o">.</span><span class="n">prod</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pdata</span> <span class="o">=</span> <span class="n">pdata</span>
		<span class="k">def</span> <span class="nf">priorprob</span><span class="p">(</span><span class="n">mu</span><span class="p">):</span>
			<span class="s">r"""
			Flat prior on the mean
			"""</span>
			<span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">prior_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">prior_range</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">priorprob</span> <span class="o">=</span> <span class="n">priorprob</span>

		<span class="k">def</span> <span class="nf">samplemu</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
			<span class="s">r"""
			Sample the parameter from the flat prior on the mean
			"""</span>
			<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">prior_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">prior_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">samplemu</span> <span class="o">=</span> <span class="n">samplemu</span>

<span class="k">def</span> <span class="nf">metropolis</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
	<span class="n">mu</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">samplemu</span><span class="p">()</span>
	<span class="n">posterior</span> <span class="o">=</span> <span class="p">[]</span>

	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
		<span class="n">new_mu</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">samplemu</span><span class="p">()</span>
		
		<span class="n">old_posterior</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pdata</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">priorprob</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
		<span class="n">new_posterior</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pdata</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">new_mu</span><span class="p">)</span><span class="o">*</span><span class="n">model</span><span class="o">.</span><span class="n">priorprob</span><span class="p">(</span><span class="n">new_mu</span><span class="p">)</span>

		<span class="n">paccept</span> <span class="o">=</span> <span class="n">new_posterior</span><span class="o">/</span><span class="n">old_posterior</span>

		<span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">paccept</span><span class="p">):</span>
			<span class="n">mu</span> <span class="o">=</span> <span class="n">new_mu</span>
		<span class="n">posterior</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

	<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
	<span class="n">Ndata</span> <span class="o">=</span> <span class="mi">100</span>
	<span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>

	<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Ndata</span><span class="p">)</span>
	<span class="n">prior_range</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span>

	<span class="n">bayes</span> <span class="o">=</span> <span class="n">BayesModel</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">prior_range</span><span class="p">)</span>

	<span class="n">posterior</span> <span class="o">=</span> <span class="n">metropolis</span><span class="p">(</span><span class="n">bayes</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>

	<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
	<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
		<span class="n">bins</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))),</span>
		<span class="n">color</span><span class="o">=</span><span class="s">'k'</span><span class="p">,</span>
		<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="n">histtype</span><span class="o">=</span><span class="s">'stepfilled'</span><span class="p">,</span>
		<span class="n">label</span><span class="o">=</span><span class="s">'Data'</span><span class="p">,</span>
		<span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
		<span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span>
		<span class="n">title_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
		<span class="n">title</span><span class="o">=</span><span class="s">r'$N_{\mathrm{data}} =$'</span> <span class="o">+</span> <span class="s">r'${}$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Data'</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$p(\mathrm{data} \sim \mathcal{N}(0, 1)$'</span><span class="p">)</span>


	<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bayes</span><span class="o">.</span><span class="n">samplemu</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span>
		<span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
		<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="n">histtype</span><span class="o">=</span><span class="s">'stepfilled'</span><span class="p">,</span>
		<span class="n">label</span><span class="o">=</span><span class="s">'Prior'</span><span class="p">,</span>
		<span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> 
		<span class="n">bins</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">posterior</span><span class="p">))),</span>
		<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
		<span class="n">histtype</span><span class="o">=</span><span class="s">'stepfilled'</span><span class="p">,</span>
		<span class="n">label</span><span class="o">=</span><span class="s">'Posterior'</span><span class="p">,</span>
		<span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
		<span class="n">loc</span><span class="o">=</span><span class="s">'upper left'</span><span class="p">,</span>
		<span class="n">title</span><span class="o">=</span><span class="s">r'$\hat{\mu} =$'</span> <span class="o">+</span> <span class="s">r'${:.2f} \pm {:.2f},$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">posterior</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">posterior</span><span class="p">))</span> <span class="o">+</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span> <span class="o">+</span> <span class="s">r'$N = {}$'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">posterior</span><span class="p">)),</span>
		<span class="n">title_fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Posterior Distribution'</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$\mu$'</span><span class="p">)</span>
	<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$p(\mu | \mathrm{data} \sim \mathcal{N}(0, 1)$'</span><span class="p">)</span>
	<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'bayes.png'</span><span class="p">)</span></code></pre></figure>

<p>Running this gives the results shown below,</p>

<p><img src="/assets/img/flatbayes.png" alt="flatBayes" /></p>

<p>We could also have run with a normally distributed prior on the mean giving,</p>

<p><img src="/assets/img/normbayes.png" alt="normBayes" /></p>

<p>We see that the choice of prior makes some difference, but the resulting posterior distribution is still in the correct region.</p>

<p><a href="/"><i class="fa fa-home" aria-hidden="true"></i> Homepage</a></p>

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

</body>
</html>

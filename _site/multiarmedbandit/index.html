<!DOCTYPE html>
<html lang="en">
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153252504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-153252504-1');
</script>

<head>
	<meta charset="utf-8">
	<title>Multi Armed Bandit Problem - James Alvey</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="James Alvey" property="og:site_name">
  
    <meta content="Multi Armed Bandit Problem" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="This is a classic reinforcement learning problem where the goal is to maximise the reward in a set of actions from a discrete choice of options. We present some of the theory and a simple example to test two different strategies." property="og:description">
  
  
    <meta content="/multiarmedbandit/" property="og:url">
  
  
    <meta content="2019-11-21T00:00:00+00:00" property="article:published_time">
    <meta content="/about/" property="article:author">
  
  
    <meta content="/assets/img/multi-armed-bandit.png" property="og:image">
  
  
    
  
  
    
    <meta content="Reinforcement Learning" property="article:tag">
    
    <meta content="Multi-Armed Bandit" property="article:tag">
    
    <meta content="Q-learning" property="article:tag">
    
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@James_A_42# add your Twitter handle">
  
    <meta name="twitter:title" content="Multi Armed Bandit Problem">
  
  
    <meta name="twitter:url" content="/multiarmedbandit/">
  
  
    <meta name="twitter:description" content="This is a classic reinforcement learning problem where the goal is to maximise the reward in a set of actions from a discrete choice of options. We present some of the theory and a simple example to test two different strategies.">
  
  
    <meta name="twitter:image:src" content="/assets/img/multi-armed-bandit.png">
  

	<meta name="description" content="This is a classic reinforcement learning problem where the goal is to maximise the reward in a set of actions from a discrete choice of options. We present some of the theory and a simple example to test two different strategies.">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body">

  <div class="wrapper">
    <aside class="sidebar">
  <header style="overflow-y: scroll;">
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/james-alvey.jpg" alt="James Alvey"></a>
      </div>
      <div class="author-name">James Alvey</div>
      <p class="about-description" align="justify">I am a PhD student in Theoretical Particle Physics and Comsology at King's College London. I am interested in how to model the early Universe and use data from Cosmology to constrain new physics.</p>
    <section class="contact">
      <ul>
        
          <li class="github"><a href="http://github.com/james-alvey-42" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="email"><a href="mailto:AlveyJBG@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
        
          <li class="linkedin"><a href="https://in.linkedin.com/in/james-alvey-a75845102" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li><a href="https://twitter.com/James_A_42# add your Twitter handle" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    </div>
    <section class="links">
      <h3 class="links-title">Links</h3>
      <ul>
          <li>Inspire-HEP Profile (Publications)</br>
            <a href="http://inspirehep.net/author/profile/J.B.G.Alvey.1" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i>inspirehep.net</a>
          </li></br>
          <li>Career (CV) </br>
            <a href="/assets/pdf/james-alvey-cv.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>PDF</a>
          </li></br>
      </ul>
    </section>
    <p></p>
    <section class="links">
      <h3 class="links-title">Articles</h3>
      <ul>
          <li><a href="/bbn" style="color: #ffffff">DarkBBN</a> <a href="http://github.com/james-alvey-42/BBN" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1910.01649.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1910.01649</a>
            <a href="https://arxiv.org/pdf/1910.10730.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1910.10730</a>
            <a href="/assets/pdf/DMUK.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>DMUK Presentation</a>
          </li></br>
          <li><a href="/boosteddm" style="color: #ffffff">Inelastic Cosmic Rays</a> <a href="http://github.com/james-alvey-42/BoostedDM" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1905.05776.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1905.05776</a>
            <a href="/assets/pdf/beam-dump.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>BUSSTEPP Presentation</a>
          </li></br>
          <li><a href="/neutrinos" style="color: #ffffff">Neutrino Astrophysics</a> <a href="http://github.com/james-alvey-42/IceCubeNeutrinos" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="https://arxiv.org/pdf/1902.01450.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>1902.01450</a>
          </li></br>
      </ul>
    </section>
    <p></p>
    <section class="links">
      <h3 class="links-title">More</h3>
      <ul>
          <li><a href="/pythonperformance" style="color: #ffffff">Free Speed in Python</a> <a href="http://github.com/james-alvey-42/ProgramTools/tree/master/PythonLessons" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a></br>
            <a href="/assets/pdf/free-speed.pdf" target="_blank"><i class="fa fa-file-pdf-o" aria-hidden="true"></i>Presentation</a>
          </li></br>
          <li><a href="/mcmc" style="color: #ffffff">Monte Carlo Markov Chains</a> <a href="http://github.com/james-alvey-42/ProgramTools/tree/master/MCMC" target="_blank"><i style="color: #ffffff;" class="fa fa-github" aria-hidden="true"></i></a>
          </li></br>
          <li>Science Communication
            <a href="https://www.varsity.co.uk/profile/james-alvey" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i>varsity.co.uk</a>
          </li></br>
      </ul>
    </section>
  </header> <!-- End Header -->
  <footer>
    <div class="copyright">
      <p>2019 &copy; James Alvey</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content">
    
    <div class="page-cover-image">
      <figure>
        <img class="page-image" src=/assets/img/multi-armed-bandit.png alt="Multi Armed Bandit Problem">
        
      </figure>
    </div> <!-- End Page Cover Image -->
    
    <div class="wrap-content">
      <header class="header-page">
        <h1 class="page-title">Multi Armed Bandit Problem</h1>
        <div class="page-date"><span>2019, Nov 21&nbsp;&nbsp;&nbsp;&nbsp;</span></div>
      </header>
      <blockquote>
  <p>This is a classic reinforcement learning problem where the goal is to maximise the reward in a set of actions from a discrete choice of options. We present some of the theory and a simple example to test two different strategies.</p>
</blockquote>

<p>This is based on an excerpt from <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">this book</a> by Sutton and Barto on Reinforcement Learning. The code for this post can be found in my Github repository at the following link:</p>

<ul>
  <li><a href="https://github.com/james-alvey-42/ReinforcementLearning/tree/master/Code/Multi-Armed-Bandit" target="blank_"><i class="fa fa-github" aria-hidden="true"></i> Reinforcement Learning Repository</a></li>
</ul>

<h2 id="statement-of-the-problem">Statement of the Problem</h2>

<p>The problem to be solved can be stated as follows. Suppose we have a bandit who can pull any one of <script type="math/tex">n</script> lever. Each lever triggers a reward from the corresponding machine. This reward is distributed according to the given parameters of the machine which are fixed throughout, but are not accessible. For simplicity say that a given machine <script type="math/tex">a</script> outputs rewards according to a normal distribution with mean <script type="math/tex">q_a</script> and variance <script type="math/tex">\sigma_a^2</script>. The goal is the find an optimal strategy to maximise the rewards recieved over <script type="math/tex">N</script> trials.</p>

<h2 id="exploitation-vs-exploration">Exploitation vs Exploration</h2>

<p>Suppose we have <script type="math/tex">M</script> machines with true mean rewards <script type="math/tex">q_a</script>, <script type="math/tex">a = 1, \ldots M</script>, then before pulling a single lever as a bandit, our knowledge of the true distributions is limited. To be more precise, our estimated <em>value</em> of each machine is independent of the machine. Clearly to proceed we should try and update this belief by pulling a variety of levers. To estimate our perceived value of machine <script type="math/tex">a</script> after some time <script type="math/tex">t</script>, we construct the following quantity,</p>

<script type="math/tex; mode=display">Q_t(a) := \frac{R_1 + R_2 + \cdots + R_{N_t(a)}}{N_t(a)}</script>

<p>where <script type="math/tex">R_i</script> is the reward received the <script type="math/tex">i</script>th time that lever <script type="math/tex">a</script> is pulled and <script type="math/tex">N_t(a)</script> is the total number of times lever <script type="math/tex">a</script> has been pulled. In other words we calculate the average reward that we have seen thus far and define that to be the current value of that lever.</p>

<p>The optimal solution then is one where these <script type="math/tex">Q_t(a)</script> converge to the true values <script type="math/tex">q(a)</script> in as few steps as possible, whilst still balancing the rewards from the “best” lever. This is the balance between <em>exploration</em> and <em>exploitation</em> - I want to check that there are no other better levers, but once I have found the best I want to stick there.</p>

<p>Now, there are a number of very sophisticated methods to solve this truly optimally, but here we consider just the simplest way to balance these two concepts:</p>

<ul>
  <li>The Greedy Method: In this case we always choose the lever we <em>currently perceive to have the highest value to us</em>.</li>
  <li>The <script type="math/tex">\epsilon</script>-Greedy Method: On the other hand, we might take the approach that <em>most</em> of the time we pull the lever that we currently think has the highest value, but some of the time randomly explore the other levers just to check we’re not missing out on anything.</li>
</ul>

<p>To put this more formally so we can write some code around it, we define a parameter <script type="math/tex">\epsilon</script> so that with probability <script type="math/tex">(1 - \epsilon)</script> we choose the action with the highest value <script type="math/tex">A_t = \mathrm{argmax} Q_t(a)</script> and with probability <script type="math/tex">\epsilon</script> we randomly choose another machine. The Greedy Method case is then where <script type="math/tex">\epsilon = 0</script>.</p>

<h2 id="implementation">Implementation</h2>

<p>To actually implement the scheme and compare the two different methods, we start by defining a <code class="highlighter-rouge">Machine</code> class which will output the rewards when it is “pulled”.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Machine</span><span class="p">():</span>
	
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>

	<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span></code></pre></figure>

<p>We also create an <code class="highlighter-rouge">Agent</code> class that has a given <script type="math/tex">\epsilon</script> parameter and number of trials.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Agent</span><span class="p">():</span>
	
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">Nsteps</span><span class="p">,</span> <span class="n">Nmachines</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Nsteps</span> <span class="o">=</span> <span class="n">Nsteps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Nmachines</span><span class="p">,</span> <span class="n">Nsteps</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Nsteps</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">numbers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Nmachines</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">averages</span> <span class="o">=</span> <span class="bp">None</span>

	<span class="k">def</span> <span class="nf">get_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">machine</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">machine</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">pull</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
		<span class="n">best_machine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
		<span class="k">if</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">):</span>
			<span class="k">return</span> <span class="n">machines</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">index_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">machines</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
			<span class="k">while</span> <span class="n">index_sample</span> <span class="o">==</span> <span class="n">best_machine</span><span class="p">:</span>
				<span class="n">index_sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">machines</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
			<span class="k">return</span> <span class="n">machines</span><span class="p">[</span><span class="n">index_sample</span><span class="p">]</span>

	<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
		<span class="n">pulled_machine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">machines</span><span class="p">)</span>
		<span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_reward</span><span class="p">(</span><span class="n">pulled_machine</span><span class="p">)</span>
		<span class="n">num</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">numbers</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">numbers</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="n">Qnew</span> <span class="o">=</span> <span class="n">Q</span>
		<span class="n">Qnew</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">Q</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span><span class="o">*</span><span class="n">num</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="k">return</span> <span class="n">Qnew</span><span class="p">,</span> <span class="n">reward</span>

	<span class="k">def</span> <span class="nf">get_averages</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span>

	<span class="k">def</span> <span class="nf">get_numbers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numbers</span></code></pre></figure>

<p>Finally, for a given agent and set of machines, we can run a full simulation of the trials with the following function:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">run_simulation</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">Nsteps</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Nsteps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
		<span class="n">Qold</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">Q</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
		<span class="n">Qnew</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">Qold</span><span class="p">,</span> <span class="n">machines</span><span class="p">)</span>
		<span class="n">agent</span><span class="o">.</span><span class="n">Q</span><span class="p">[:,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Qnew</span>
		<span class="n">agent</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
	<span class="n">averages</span> <span class="o">=</span> <span class="p">[]</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">rewards</span><span class="p">)):</span>
		<span class="n">averages</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">rewards</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">i</span><span class="p">))</span>
	<span class="n">agent</span><span class="o">.</span><span class="n">averages</span> <span class="o">=</span> <span class="n">averages</span></code></pre></figure>

<p>The end of this function computes the rolling averages of the total reward gained whcih can then be plotted. To implement these and trial a few strategies, we just need:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Nmachines</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Nmachines</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Nmachines</span><span class="p">)</span>
<span class="n">machines</span> <span class="o">=</span> <span class="p">[</span><span class="n">Machine</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">means</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Nmachines</span><span class="p">)]</span>
<span class="n">Nsteps</span> <span class="o">=</span> <span class="mi">10000</span>

<span class="n">agent1</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">Nmachines</span><span class="o">=</span><span class="n">Nmachines</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0.1'</span><span class="p">)</span>
<span class="n">agent2</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">Nmachines</span><span class="o">=</span><span class="n">Nmachines</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0.01'</span><span class="p">)</span>
<span class="n">agent3</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">Nmachines</span><span class="o">=</span><span class="n">Nmachines</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'0.0'</span><span class="p">)</span>

<span class="n">run_simulation</span><span class="p">(</span><span class="n">agent1</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">machines</span><span class="o">=</span><span class="n">machines</span><span class="p">)</span>
<span class="n">run_simulation</span><span class="p">(</span><span class="n">agent2</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">machines</span><span class="o">=</span><span class="n">machines</span><span class="p">)</span>
<span class="n">run_simulation</span><span class="p">(</span><span class="n">agent3</span><span class="p">,</span> <span class="n">Nsteps</span><span class="o">=</span><span class="n">Nsteps</span><span class="p">,</span> <span class="n">machines</span><span class="o">=</span><span class="n">machines</span><span class="p">)</span></code></pre></figure>

<p>With some plotting we find the following results:</p>

<p><img src="/assets/img/multi-armed-bandit-full.png" alt="fullresults" /></p>

<h2 id="conclusions-epsilon-greedy-policies">Conclusions: <script type="math/tex">\epsilon</script>-greedy policies</h2>

<p>If we take a look at the results, we see that strategies with non-zero <script type="math/tex">\epsilon</script> work significantly better than there purely greedy strategy. We can also understand why this is the case: the greedy strategy doesn’t bother discovering lever 7, whilst the ones that do explore find it at various points and then take advantage thereafter. Indeed we see that <script type="math/tex">\epsilon = 0.01</script> takes longer to find the lever, but as soon as it does it catches up with <script type="math/tex">\epsilon = 0.1</script> realtively quickly once it does.</p>

<p>This is of course a brief interlude into this problem, but it illustrates the key ideas and provides a simple implementation. A more thorough analysis might take into account a suite of simulations for different distributions of means and variances for the machines to see which strategy is globally the best.</p>

<h2 id="gradient-bandits">Gradient Bandits</h2>

<p>Suppose instead of estimating the <em>actual</em> value of an action <script type="math/tex">a</script> at some time <script type="math/tex">t</script> (<script type="math/tex">Q_t(a)</script>), we now consider learning a <em>preference</em> <script type="math/tex">H_t(a)</script>. For some set of preferences, we can then choose an action based on the probability distribution:</p>

<script type="math/tex; mode=display">\pi_t(a) = \mathbb{P}(A_t = a) = \frac{\exp[H_t(a)]}{\sum_{b}{\exp[H_t(b)]}}</script>

<p>This has the advantage that it removes an initial bias on the expected value of a reward. If we initially give a flat prior to all of the machines, then we can update our preferences according to,</p>

<script type="math/tex; mode=display">H_{t + 1}(A_t) = H_t(A_t) + \alpha (R_t - \bar{R}_t) (1 - \pi_t(A_t)</script>

<script type="math/tex; mode=display">H_{t + 1}(a) = H_t(a) - \alpha (R_t - \bar{R}_t) \pi_t(a) \quad \forall a \neq A_t</script>

<p>where <script type="math/tex">R_t</script> is the reward received at timestep <script type="math/tex">t</script>, <script type="math/tex">\bar{R}_t</script> is the average reward received over all times previous to <script type="math/tex">t</script>, <script type="math/tex">A_t</script> is the action taken at time <script type="math/tex">t</script>, and <script type="math/tex">\alpha</script> is a parameter than controls the step size. At this point, these definitions look a little arbitrary. It is nice that we have the notion of a probability distribution, but why should we update it in the way given above? The derivation is in <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Sutton and Barto</a>, but the key points are the following:</p>

<ul>
  <li>
    <p>The update preserves the total preference i.e. <script type="math/tex">\sum_{a}{H_t(a)} = \mathrm{const.}</script>. In other words, the preferences just get reshared out from one timestep to the next. To show this, you need the fact that the probabilities add to unity.</p>
  </li>
  <li>
    <p>It can be shown that the update above is the sampling equivalent to maximising the expected reward <script type="math/tex">\mathbb{E}(R_t)</script> over the probability distribution <script type="math/tex">\pi</script>. If you believe me that this is indeed the case, then the above is equivalent to;</p>
  </li>
</ul>

<script type="math/tex; mode=display">H_{t + 1}(a) = H_t(a) + \alpha \frac{\partial \mathbb{E}(R_t)}{\partial H_t(a)}</script>

<p>which is just a very simple form of gradient ascent.</p>

<h2 id="implementation-1">Implementation</h2>

<p>To implement the above we define a new class <code class="highlighter-rouge">ProbabilityAgent</code>;</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ProbabilityAgent</span><span class="p">():</span>
	
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Nsteps</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">Nmachines</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">Nsteps</span> <span class="o">=</span> <span class="n">Nsteps</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">label</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Nmachines</span><span class="p">,</span> <span class="n">Nsteps</span><span class="p">))</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span>  <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">Nmachines</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">Nmachines</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Nsteps</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">numbers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Nmachines</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">averages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Nsteps</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">get_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">machine</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">machine</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

	<span class="k">def</span> <span class="nf">pull</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
		<span class="n">machine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">machines</span><span class="p">))],</span> 
									<span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
									<span class="n">p</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span><span class="p">))</span>
		<span class="k">return</span> <span class="n">machines</span><span class="p">[</span><span class="n">machine</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

	<span class="k">def</span> <span class="nf">first_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
		<span class="n">pulled_machine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="n">machines</span><span class="p">)</span>
		<span class="n">reward</span> <span class="o">=</span> <span class="n">pulled_machine</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">numbers</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
		<span class="n">machine_index</span> <span class="o">=</span> <span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">machines</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'bool'</span><span class="p">)</span>
		<span class="n">mask</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
		<span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
		<span class="n">H</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
		<span class="n">H</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span><span class="p">[</span><span class="n">machine_index</span><span class="p">])</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">calculate_probabilities</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">+=</span> <span class="mi">1</span>

	<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">machines</span><span class="p">):</span>
		<span class="n">pulled_machine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="n">machines</span><span class="p">)</span>
		<span class="n">reward</span> <span class="o">=</span> <span class="n">pulled_machine</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">numbers</span><span class="p">[</span><span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
		<span class="n">machine_index</span> <span class="o">=</span> <span class="n">pulled_machine</span><span class="o">.</span><span class="n">label</span>
		<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">machines</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'bool'</span><span class="p">)</span>
		<span class="n">mask</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
		<span class="n">H</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span><span class="p">]</span>
		<span class="n">H</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
		<span class="n">H</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span><span class="p">[</span><span class="n">machine_index</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span><span class="p">[</span><span class="n">machine_index</span><span class="p">])</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">H</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">calculate_probabilities</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">pulls</span> <span class="o">+=</span> <span class="mi">1</span>

	<span class="k">def</span> <span class="nf">get_averages</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">averages</span>

	<span class="k">def</span> <span class="nf">get_numbers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numbers</span></code></pre></figure>

<p>Running the simulations in the same case as the <script type="math/tex">\epsilon</script>-greedy bandit, we find the following results;</p>

<p><img src="/assets/img/multi-armed-bandit-h.png" alt="h-results" /></p>

<h2 id="conclusions-gradient-bandit">Conclusions: Gradient Bandit</h2>

<p>We see from the results that the convergence is quite sensitive to <script type="math/tex">\alpha</script>. Furthemore, we see that we indeed find a better strategy compared to the <script type="math/tex">\epsilon</script>-greedy case, with a higher average reward. One thing that we did not test here is whether this secondary method performs better with different initialisations of the preferences compared to varying the initial action value priors. One would expect as such, since the action priors can be biased if the order of magnitude expected for the reward is not known.</p>

<p><a href="/"><i class="fa fa-home" aria-hidden="true"></i> Homepage</a></p>

    </div> <!-- End Wrap Content -->
  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

</body>
</html>

I"<blockquote>
  <p>This is a classic reinforcement learning problem where the goal is to maximise the reward in a set of actions from a discrete choice of options. We present some of the theory and a simple example to test two different strategies.</p>
</blockquote>

<p>This is based on an excerpt from <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">this book</a> by Sutton and Barto on Reinforcement Learning. The code for this post can be found in my Github repository at the following link:</p>

<ul>
  <li><a href="https://github.com/james-alvey-42/ReinforcementLearning/tree/master/Code/Multi-Armed-Bandit" target="blank_"><i class="fa fa-github" aria-hidden="true"></i> Reinforcement Learning Repository</a></li>
</ul>

<h2 id="statement-of-the-problem">Statement of the Problem</h2>

<p>The problem to be solved can be stated as follows. Suppose we have a bandit who can pull any one of <script type="math/tex">n</script> lever. Each lever triggers a reward from the corresponding machine. This reward is distributed according to the given parameters of the machine which are fixed throughout, but are not accessible. For simplicity say that a given machine <script type="math/tex">a</script> outputs rewards according to a normal distribution with mean <script type="math/tex">q_a</script> and variance <script type="math/tex">\sigma_a^2</script>. The goal is the find an optimal strategy to maximise the rewards recieved over <script type="math/tex">N</script> trials.</p>

<h2 id="exploitation-vs-exploration">Exploitation vs Exploration</h2>

<p>Suppose we have <script type="math/tex">M</script> machines with true mean rewards <script type="math/tex">q_a</script>, <script type="math/tex">a = 1, \ldots M</script>, then before pulling a single lever as a bandit, our knowledge of the true distributions is limited. To be more precise, our estimated <em>value</em> of each machine is independent of the machine. Clearly to proceed we should try and update this belief by pulling a variety of levers. To estimate our perceived value of machine <script type="math/tex">a</script> after some time <script type="math/tex">t</script>, we construct the following quantity,</p>

<script type="math/tex; mode=display">Q_t(a) := \frac{R_1 + R_2 + \cdots + R_{N_t(a)}}{N_t(a)}</script>

<p>where <script type="math/tex">R_i</script> is the reward received the <script type="math/tex">i</script>th time that lever <script type="math/tex">a</script> is pulled and <script type="math/tex">N_t(a)</script> is the total number of times lever <script type="math/tex">a</script> has been pulled. In other words we calculate the average reward that we have seen thus far and define that to be the current value of that lever.</p>

<p>The optimal solution then is one where these <script type="math/tex">Q_t(a)</script> converge to the true values <script type="math/tex">q(a)</script> in as few steps as possible, whilst still balancing the rewards from the “best” lever. This is the balance between <em>exploration</em> and <em>exploitation</em> - I want to check that there are no other better levers, but once I have found the best I want to stick there.</p>

<p>Now, there are a number of very sophisticated methods to solve this truly optimally, but here we consider just the simplest way to balance these two concepts:</p>

<ul>
  <li>The Greedy Method: In this case we always choose the lever we <em>currently perceive to have the highest value to us</em>.</li>
  <li>The <script type="math/tex">\epsilon</script>-Greedy Method: On the other hand, we might take the approach that <em>most</em> of the time we pull the lever that we currently think has the highest value, but some of the time randomly explore the other levers just to check we’re not missing out on anything.</li>
</ul>

<p>To put this more formally so we can write some code around it, we define a parameter <script type="math/tex">\epsilon</script> so that with probability <script type="math/tex">(1 - \epsilon)</script> we choose the action with the highest value <script type="math/tex">A_t = \mathrm{argmax} Q_t(a)</script> and with probability <script type="math/tex">\epsilon</script> we randomly choose another machine. The Greedy Method case is then where <script type="math/tex">\epsilon = 0</script>.</p>

<p>## Implementation</p>

:ET
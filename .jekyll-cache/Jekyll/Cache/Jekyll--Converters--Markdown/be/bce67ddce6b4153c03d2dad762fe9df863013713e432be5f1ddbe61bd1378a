I"¹_<p>Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of <em>sampling</em>.</p>

<h2 id="the-sampling-problem">The Sampling Problem</h2>

<p>We can state the sampling problem as follows;</p>

<blockquote>
  <p>Let <script type="math/tex">\mathcal{D}</script> be a distribution over a finite set <script type="math/tex">X</script>. Further, we assume that we have access to <script type="math/tex">p(x)</script> for some <script type="math/tex">x in X</script> which outputs the probabiliity of drawing <script type="math/tex">x</script> given <script type="math/tex">\mathcal{D}</script>. The sampling problem is to design an algorithm <script type="math/tex">\mathcal{A}</script> which outputs an element of <script type="math/tex">x</script> approximately with probability <script type="math/tex">p(x)</script>.</p>
</blockquote>

<p>The reason this is a problem is the following: suppose you generate a large number of choices <script type="math/tex">\{x_0, x_1, x_2, \cdots\}</script> from the underlying set according to some distribtuion that is <em>not</em> <script type="math/tex">p(x)</script>. As it stands, this is certainly not a sample that is close to being representative of the underlying distribution. To generate a genuine sample from this selection, we have to apply some criterion to each point. Note that we have access to <script type="math/tex">\{p(x_0), p(x_1), \cdots\}</script>, so this criterion might take the form of the following:</p>

<ol>
  <li>Simulate the list <script type="math/tex">\{x_0, x_1, \cdots\}</script></li>
  <li>For each data point <script type="math/tex">x_k</script>, compute <script type="math/tex">p(x_k)</script></li>
  <li>Generate a random number <script type="math/tex">p \sim U[0, 1]</script></li>
  <li>If <script type="math/tex">% <![CDATA[
p < p(x_k) %]]></script>, add the point <script type="math/tex">x_k</script> to the sample, else reject the point</li>
</ol>

<p>The probelm with this is that for a large set <script type="math/tex">X</script>, there might only be a small region where the probability is non-neglible, so the vast majority of the points in the original list will be rejected. The above approach will generate a sample that reflects the underlying distribution, the problem is that it will be very slow. To show that this really does work, consider the following example.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">N</span>         <span class="o">=</span> <span class="mf">1e6</span>
<span class="n">x</span>         <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">p</span>         <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">p_test</span>    <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_test</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">)</span>
<span class="n">sample</span>    <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">criterion</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r'$x$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">r'$p(x)$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">sample</span><span class="p">],</span> 
	<span class="n">bins</span><span class="o">=</span><span class="mf">1e2</span><span class="p">,</span> 
	<span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
	<span class="n">histtype</span><span class="o">=</span><span class="s">'step'</span><span class="p">,)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

<p>This produces the graphical output below. Testing the code, we find that approximately 90% of the original sample is rejected, illustrating the issue with the method to generate samples of a given size.</p>

<p><img src="/assets/img/easy_sample.png" alt="Easy Sample" /></p>

<blockquote>
  <p>Using the easy sampling method, the probability criterion is applied to the blue sample, resulting in the correct green sample for a normal distribution with mean 0, and variance 1.*</p>
</blockquote>

<h2 id="the-solution-markov-chains">The Solution: Markov Chains</h2>

<p>To solve this sampling problem, we turn to the second MC in the title, <em>Markov Chains</em>. We can view a Markoc Chain as a random walk on a graph in that sense that for a given graph of vertices and edges <script type="math/tex">G = (V, E)</script>, we should specify a number <script type="math/tex">p_{uv} \in [0, 1]</script> for each edge <script type="math/tex">e = (u, v) \in E</script>. For this to be a true random walk, the probability should satisfy the condition that for every vertex <script type="math/tex">v \in V</script>, <script type="math/tex">\sum_y{p_{xy}} = 1</script> where the sum is over all outgoing edges. In other words, the outgoing values from the vertex form a probability distribution. To proceed, we need to appeal to the <em>Fundamental Theorem of Markov Chains</em> which states the following:</p>

<h3 id="fundamental-theorem-of-markov-chains">Fundamental Theorem of Markov Chains</h3>

<blockquote>
  <p>For any irreducible, aperiodic, positive-recurrent Markov Chain there exists a <em>unique stationary distribution</em> <script type="math/tex">\pi_j, j \in \mathbb{Z}</script>. Intuitively this says that the probability you end up on a given vertex is independent of where you start, and that the given distribution is uniquely determined by the Markov Chain.</p>
</blockquote>

<p>Now, to introduce the stationary distribution, we will represent the transition probabilities between states <script type="math/tex">i</script> and <script type="math/tex">j</script> as entries in a matrix <script type="math/tex">A = (A_{ij})</script>. The stationary distribution <script type="math/tex">\pi</script> then satisfies <script type="math/tex">A \pi = \pi</script> i.e. it is an <em>eigenvector</em> of <script type="math/tex">A</script> with eigenvalue 1. To guarantee the existence of a unique such vector, there are necessarily conditions on the matrix, or equivalently on the Markov Chains, but these will be satisfied by construction in the case at hand.</p>

<h3 id="constructing-a-graph-to-wak-on">Constructing a Graph to Wak On</h3>

<p>We are now in a position to understand the MCMC method;</p>

<blockquote>
  <p>The MCMC method is as follows: we want to sample over a finite set <script type="math/tex">X</script> with probability function <script type="math/tex">p(x)</script>. To do so, we construct a Markov Chain whose stationary distribution is exactly <script type="math/tex">p</script>. This is equivalent to choosing a graph and a set of transition probabilities. The sample is then generated by performing a random walk on the graph and listing the vertices. In the long term, the time spent at each vertex will be proportional to the stationary distribution.</p>
</blockquote>

<p>Now we can construct the probability distribution on a given lattice <script type="math/tex">\{0, 1, \cdots, n\}^d</script>. Then, let <script type="math/tex">r = 2d</script> and suppose we are at some vertex <script type="math/tex">i</script>. To choose where to go next we do the following;</p>

<ol>
  <li>Pick a neighbouring vertex <script type="math/tex">j</script> with probability <script type="math/tex">1/r</script>, else stay at <script type="math/tex">i</script></li>
  <li>If you pick <script type="math/tex">j</script> <em>and</em> <script type="math/tex">p(j) \geq p(i)</script>, go to <script type="math/tex">j</script> deterministically</li>
  <li>Otherwise go to <script type="math/tex">j</script> with probability <script type="math/tex">p(j)/p(i)</script>.</li>
</ol>

<p>To prove that this really is stationary, we note that we can write,</p>

<script type="math/tex; mode=display">p_{i, j} = \frac{1}{r} \mathrm{min}\left(1, p(j)/p(i)\right), \quad p_{i, i}=1-\sum_{(i, j) \in E(G) ; j \neq i} p_{i, j}</script>

<p>Then use the fact that if a probability distribution, <script type="math/tex">\pi(x)</script> is stationary, then <script type="math/tex">\pi(x)p_{x, y} = \pi(y)p_{y, x}</script> (this is the statement of detailed balance). Summing over the right hand side this gives exactly <script type="math/tex">\pi(x) = \pi(y)p_{y, x}</script> where we sum over <script type="math/tex">y</script> implicitly. This is indeed the eigenvalue equation. Doing this with our choice of probability distribution, we can work only with the first expression and note that,</p>

<script type="math/tex; mode=display">p(i)p_{i, j} = p(j)p_{j, i} = \frac{1}{r}\mathrm{min}\left(p(i), p(j)\right)</script>

<p>So, we have found a graph that has a suitable stationary distribution. To generate a sample, we simply randomly choose a starting point (maybe after jumping round the graph a bit) and compute a random walk on the graph according to these probabilities.</p>

<h2 id="an-example-on-mathbbz">An Example on <script type="math/tex">\mathbb{Z}</script></h2>

<p>Consider the following probability distribution defined on the integers,</p>

<script type="math/tex; mode=display">% <![CDATA[
p(k) = \begin{cases} \frac{3}{\pi^2 + 3} & k = 0 \\  \frac{3}{\pi^2 + 3}\frac{1}{k^2} & k \neq 0 \end{cases} %]]></script>

<p>This is normalised thanks to the result that <script type="math/tex">\sum_{k = 0}^{\infty}{k^{-2}} = \pi^2/6</script>. We define this as well as the relevant Metropolis-Hastings algorithm in the following functions,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
	<span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span>
	<span class="k">else</span><span class="p">:</span>
		<span class="k">return</span> <span class="p">(</span><span class="mi">3</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">metropolis</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
	<span class="c1"># Choose a random integer starting at zero
</span>	<span class="n">k</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="n">sample</span> <span class="o">=</span> <span class="p">[]</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
		<span class="c1"># Choose neighbour with probability 1/2
</span>		<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
			<span class="n">neighbour</span> <span class="o">=</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="n">neighbour</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="mi">1</span>
		<span class="c1"># Go to neighbour deterministically if p(neighbour) &gt; p(k)
</span>		<span class="k">if</span> <span class="n">p</span><span class="p">(</span><span class="n">neighbour</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
			<span class="n">k</span> <span class="o">=</span> <span class="n">neighbour</span>
		<span class="k">else</span><span class="p">:</span>
			<span class="c1"># Go to neighbour with probability p(neighbour)/p(k)
</span>			<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">(</span><span class="n">neighbour</span><span class="p">)</span><span class="o">/</span><span class="n">p</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
				<span class="n">k</span> <span class="o">=</span> <span class="n">neighbour</span>
		<span class="n">sample</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span></code></pre></figure>

<p>We are now in a position to test our algorithm. We find the results shown below for <script type="math/tex">N =</script> and <script type="math/tex">N =</script>. There are a couple of observations to make. Firstly, we see that as we increase <script type="math/tex">N</script>, the random walk explores more of the graph and represents the stationary distribution better. On the other hand, due to the fact that in order to explore the extremities, there is a small probability of going back. Hence we see that the results tend to be biased towards one direction.</p>

<p><img src="/assets/img/smallN.png" alt="smallN" /> <img src="/assets/img/largeN.png" alt="largeN" /></p>

<p>To counter this latter problem, we can instead take multiple samples using the Metropolis algorithm and combine them to form one large sample. This will ensure that although each sample is likely to be biased towards positive or negative values, the total sample will not be. This has the additional benefit of being easily parallelisable using Pythonâs <a href="https://joblib.readthedocs.io/en/latest/">joblib library</a> which is implemented as follows,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="n">N</span><span class="p">,</span> <span class="n">Ns</span> <span class="o">=</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">1e2</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">delayed</span><span class="p">(</span><span class="n">metropolis</span><span class="p">)(</span><span class="n">N</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">Ns</span><span class="p">))))</span></code></pre></figure>

<p>This gives much more symmetrical results that nonetheless explore much of the feature space.</p>

<p>.. figure:: ../MCMC/betterN.png
    :align: center
    :alt: Better Results</p>

<h2 id="another-example-the-double-gaussian">Another Example: The Double Gaussian</h2>

<p>To further illustrate the issue of not exploring the full parameter space, we consider the example of a continuous random variable whose p.d.f. is defined by,</p>

<p>.. math:: f(x; \mu, \sigma^2) = \frac{1}{2\sqrt{2\pi}\sigma} \left[\exp\left(\frac{(x - \mu)^2}{2\sigma^2}\right) + \exp\left(\frac{(x + \mu)^2}{2\sigma^2}\right)\right]</p>

<p>which is essentially a sum of two Gaussians centred at <script type="math/tex">\pm \mu</script>. We now run the continuous equivalent of the Metropolis-Hastings algorithm for this double Gaussian with <script type="math/tex">\mu = 0</script> and <script type="math/tex">\mu = 2.0</script>. This is shown in the figure below, where we see that indeed in the latter case, the algorithm gets stuck on one side of the distribution, unable to jump to the other maxima.</p>

<table>
  <tbody>
    <tr>
      <td>bad</td>
      <td>Â </td>
      <td>badjoint</td>
    </tr>
  </tbody>
</table>

<p>.. |bad| image:: ../MCMC/MCMC_bad.png
   :width: 45%</p>

<p>.. |badjoint| image:: ../MCMC/MCMCjoint_bad.png
   :width: 53%</p>

<p>Taking the same approach as in the discrete case, we now parallelise the sampling and concatenate the result. This leads to much better performance as shown below.</p>

<table>
  <tbody>
    <tr>
      <td>good</td>
      <td>Â </td>
      <td>goodjoint</td>
    </tr>
  </tbody>
</table>

<p>.. |good| image:: ../MCMC/MCMC_good.png
   :width: 45%</p>

<p>.. |goodjoint| image:: ../MCMC/MCMCjoint_good.png
   :width: 53%</p>

<p>The full code for this example is given below:</p>

<p>.. literalinclude:: ../MCMC/mcmc.py</p>

<h2 id="bayesian-parameter-estimation">Bayesian Parameter Estimation</h2>

<p>THe other major use of Monte Carlo Markov Chains is in Bayesian parameter estimation. A prominent example of this is in Cosmology, where <script type="math/tex">\Lambda\mathrm{CDM}</script> models are fitted to CMB/Galaxy survey/weak lensing data. In this case, instead of sampling from a distribution, the aim is to estimate the posterior distribution (i.e. generate a representative sample that approximately represents the underlying posterior). This process is encoded in Bayesâ rule; suppose we have some data <script type="math/tex">X = \{X_0, X_1, X_2, \cdots\}</script> and a statistical model that tells us the probabiltiy of a given realisation of the data given some set of parameters <script type="math/tex">\theta = (\theta_1, \theta_2, \cdots)</script>. Suppose further that we also have a <em>prior</em> view on the value of the parameters, denoted <script type="math/tex">p(\theta)</script>. Bayesâ theorem then lets us calculate the probability that the set of parameters are the true parameters given the data, the <em>posterior</em> distribution;</p>

<table>
  <tbody>
    <tr>
      <td>.. math:: p(\theta</td>
      <td>X) = \frac{p(X</td>
      <td>\theta)p(\theta)}{P(X)}, \quad p(X) = \int{\mathrm{d}\mu_\theta p(X</td>
      <td>\theta) p(\theta)}</td>
    </tr>
  </tbody>
</table>

<p>Now, the reason MCMC methods are useful in this case is that the denominator of this fraction is difficult to calculate in general especially for complex models. Since the MCMC only requires ratios of posterior probabilities however, this difficult factor cancels out in expressions such as,</p>

<table>
  <tbody>
    <tr>
      <td>.. math:: \frac{p(\theta_i</td>
      <td>X)}{p(\theta_j</td>
      <td>X)} = \frac{p(X</td>
      <td>\theta_i) p(\theta_i)}{p(X</td>
      <td>\theta_j) p(\theta_j)}</td>
    </tr>
  </tbody>
</table>

<p>which is essentially a likelihood ratio. Now we can simply follow the Metropolis-Hastings algorithm to use a Monte Carlo Markov Chain to sample from the posterior distribution. This has the benefit that we can estimate not only the best-fit parameters, but also an error in these parameters since we have access to the full distribution. We can implement the algorithm as follows,</p>

<ol>
  <li>Sample a random point in the parameter space <script type="math/tex">\theta_i</script> according to the prior</li>
  <li>Sample another random point in the parameter space <script type="math/tex">\theta_j</script></li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Compute $$\ell(X, \theta_i) = p(X</td>
          <td>\theta_i)p(\theta_i)<script type="math/tex">and</script>\ell(X, \theta_j) = p(X</td>
          <td>\theta_j)p(\theta_j)$$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>If <script type="math/tex">\ell(X, \theta_j) > \ell(X, \theta_i)</script>, keep the new value, <script type="math/tex">\theta_j</script>, then repeat, else keep the old value with probabiltiy <script type="math/tex">1 - \ell(X, \theta_j)/\ell(X, \theta_i)</script></li>
</ol>

<p>This final step can be rewritten in terms of the log-likelihood. If we accept the new parameter values with probability <script type="math/tex">p \sim U[0, 1]</script>, then we accept the update if,</p>

<table>
  <tbody>
    <tr>
      <td>.. math:: \log p &lt; \sum_{i \in \mathrm{data}}{\left[\log p(x_i</td>
      <td>\theta_j) - \log p(x_i</td>
      <td>\theta_i)\right]} + \log p(\theta_j) - \log p(\theta_i)</td>
    </tr>
  </tbody>
</table>

<p>An Example: Fitting a Normal Distribution
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p>

<p>We take possibly the simplest example and consider data distributed according to a standard normal distribution, <script type="math/tex">X \sim \mathcal{N}(0, 1)</script>. Then we want to find the posterior distribution on the mean, <script type="math/tex">\mu</script>, for fixed variance. The code to implement this is shown below in the case of a flat prior,</p>

<p>.. literalinclude:: ../MCMC/bayes.py</p>

<p>Running this gives the results shown below,</p>

<p>.. image:: ../MCMC/flatbayes.png</p>

<p>We could also have run with a normally distributed prior on the mean giving,</p>

<p>.. image:: ../MCMC/normbayes.png</p>

<p>We see that the choice of prior makes some difference, but the resulting posterior distribution is still in the correct region.</p>
:ET
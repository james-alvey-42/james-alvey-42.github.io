---
layout: post
title: Monte Carlo Markov Chains
date: 2019-08-20 00:00:00
description: Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of *sampling*.
img: monte-carlo.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Monte Carlo Markov Chains, MCMC, Python]
---

Monte Carlo Markov Chains (MCMC) have many applications across physics and statistics in the estimation of parameters and uncertainties given data in a Bayesian framework. The underlying use of the Markov Chain, however, is to solve the more fundamental issue of *sampling*.

## The Sampling Problem

We can state the sampling problem as follows;

> Let $$\mathcal{D}$$ be a distribution over a finite set $$X$$. Further, we assume that we have access to $$p(x)$$ for some $$x in X$$ which outputs the probabiliity of drawing $$x$$ given $$\mathcal{D}$$. The sampling problem is to design an algorithm $$\mathcal{A}$$ which outputs an element of $$x$$ approximately with probability $$p(x)$$.


The reason this is a problem is the following: suppose you generate a large number of choices $$\{x_0, x_1, x_2, \cdots\}$$ from the underlying set according to some distribtuion that is *not* $$p(x)$$. As it stands, this is certainly not a sample that is close to being representative of the underlying distribution. To generate a genuine sample from this selection, we have to apply some criterion to each point. Note that we have access to $$\{p(x_0), p(x_1), \cdots\}$$, so this criterion might take the form of the following:

1. Simulate the list $$\{x_0, x_1, \cdots\}$$
2. For each data point $$x_k$$, compute $$p(x_k)$$
3. Generate a random number $$p \sim U[0, 1]$$
4. If $$p < p(x_k)$$, add the point $$x_k$$ to the sample, else reject the point

The probelm with this is that for a large set $$X$$, there might only be a small region where the probability is non-neglible, so the vast majority of the points in the original list will be rejected. The above approach will generate a sample that reflects the underlying distribution, the problem is that it will be very slow. To show that this really does work, consider the following example.

{%highlight python%}

import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

N         = 1e6
x         = np.random.uniform(-5, 5, N)
p         = norm.pdf(x)
p_test    = np.random.uniform(0, 1, N)
criterion = (p_test < p)
sample    = x[criterion]

plt.figure()
plt.xlabel(r'$x$')
plt.ylabel(r'$p(x)$')
plt.hist([x, sample], 
	bins=1e2, 
	density=True,
	histtype='step',)
plt.show()

{%endhighlight%}

This produces the graphical output below. Testing the code, we find that approximately 90% of the original sample is rejected, illustrating the issue with the method to generate samples of a given size.

![Easy Sample]({{site.baseurl}}/assets/img/easy_sample.png)

>Using the easy sampling method, the probability criterion is applied to the blue sample, resulting in the correct green sample for a normal distribution with mean 0, and variance 1.*

## The Solution: Markov Chains

To solve this sampling problem, we turn to the second MC in the title, *Markov Chains*. We can view a Markoc Chain as a random walk on a graph in that sense that for a given graph of vertices and edges $$G = (V, E)$$, we should specify a number $$p_{uv} \in [0, 1]$$ for each edge $$e = (u, v) \in E$$. For this to be a true random walk, the probability should satisfy the condition that for every vertex $$v \in V$$, $$\sum_y{p_{xy}} = 1$$ where the sum is over all outgoing edges. In other words, the outgoing values from the vertex form a probability distribution. To proceed, we need to appeal to the *Fundamental Theorem of Markov Chains* which states the following:

### Fundamental Theorem of Markov Chains

> For any irreducible, aperiodic, positive-recurrent Markov Chain there exists a *unique stationary distribution* $$\pi_j, j \in \mathbb{Z}$$. Intuitively this says that the probability you end up on a given vertex is independent of where you start, and that the given distribution is uniquely determined by the Markov Chain.

Now, to introduce the stationary distribution, we will represent the transition probabilities between states $$i$$ and $$j$$ as entries in a matrix $$A = (A_{ij})$$. The stationary distribution $$\pi$$ then satisfies $$A \pi = \pi$$ i.e. it is an *eigenvector* of $$A$$ with eigenvalue 1. To guarantee the existence of a unique such vector, there are necessarily conditions on the matrix, or equivalently on the Markov Chains, but these will be satisfied by construction in the case at hand.

### Constructing a Graph to Wak On

We are now in a position to understand the MCMC method;

> The MCMC method is as follows: we want to sample over a finite set $$X$$ with probability function $$p(x)$$. To do so, we construct a Markov Chain whose stationary distribution is exactly $$p$$. This is equivalent to choosing a graph and a set of transition probabilities. The sample is then generated by performing a random walk on the graph and listing the vertices. In the long term, the time spent at each vertex will be proportional to the stationary distribution.

Now we can construct the probability distribution on a given lattice $$\{0, 1, \cdots, n\}^d$$. Then, let $$r = 2d$$ and suppose we are at some vertex $$i$$. To choose where to go next we do the following;

1. Pick a neighbouring vertex $$j$$ with probability $$1/r$$, else stay at $$i$$
2. If you pick $$j$$ *and* $$p(j) \geq p(i)$$, go to $$j$$ deterministically
3. Otherwise go to $$j$$ with probability $$p(j)/p(i)$$.

To prove that this really is stationary, we note that we can write,

$$p_{i, j} = \frac{1}{r} \mathrm{min}\left(1, p(j)/p(i)\right), \quad p_{i, i}=1-\sum_{(i, j) \in E(G) ; j \neq i} p_{i, j}$$

Then use the fact that if a probability distribution, $$\pi(x)$$ is stationary, then $$\pi(x)p_{x, y} = \pi(y)p_{y, x}$$ (this is the statement of detailed balance). Summing over the right hand side this gives exactly $$\pi(x) = \pi(y)p_{y, x}$$ where we sum over $$y$$ implicitly. This is indeed the eigenvalue equation. Doing this with our choice of probability distribution, we can work only with the first expression and note that,

$$p(i)p_{i, j} = p(j)p_{j, i} = \frac{1}{r}\mathrm{min}\left(p(i), p(j)\right)$$

So, we have found a graph that has a suitable stationary distribution. To generate a sample, we simply randomly choose a starting point (maybe after jumping round the graph a bit) and compute a random walk on the graph according to these probabilities.

## An Example on $$\mathbb{Z}$$

Consider the following probability distribution defined on the integers,

$$p(k) = \begin{cases} \frac{3}{\pi^2 + 3} & k = 0 \\  \frac{3}{\pi^2 + 3}\frac{1}{k^2} & k \neq 0 \end{cases}$$

This is normalised thanks to the result that $$\sum_{k = 0}^{\infty}{k^{-2}} = \pi^2/6$$. We define this as well as the relevant Metropolis-Hastings algorithm in the following functions,

{%highlight python%}
def p(k):
	if k == 0:
		return (3/(np.power(np.pi, 2) + 3))
	else:
		return (3/(np.power(np.pi, 2) + 3)) * np.power(k, -2.0)

def metropolis(N):
	# Choose a random integer starting at zero
	k = 0
	sample = []
	for _ in range(int(N)):
		# Choose neighbour with probability 1/2
		if np.random.uniform(0, 1) < 0.5:
			neighbour = k + 1
		else:
			neighbour = k - 1
		# Go to neighbour deterministically if p(neighbour) > p(k)
		if p(neighbour) >= p(k):
			k = neighbour
		else:
			# Go to neighbour with probability p(neighbour)/p(k)
			if np.random.uniform(0, 1) < p(neighbour)/p(k):
				k = neighbour
		sample.append(k)
	return np.array(sample)
{%endhighlight%}

We are now in a position to test our algorithm. We find the results shown below for $$N = $$ and $$N = $$. There are a couple of observations to make. Firstly, we see that as we increase $$N$$, the random walk explores more of the graph and represents the stationary distribution better. On the other hand, due to the fact that in order to explore the extremities, there is a small probability of going back. Hence we see that the results tend to be biased towards one direction.

![smallN]({{site.baseurl}}/assets/img/smallN.png) ![largeN]({{site.baseurl}}/assets/img/largeN.png)

To counter this latter problem, we can instead take multiple samples using the Metropolis algorithm and combine them to form one large sample. This will ensure that although each sample is likely to be biased towards positive or negative values, the total sample will not be. This has the additional benefit of being easily parallelisable using Python's [joblib library](https://joblib.readthedocs.io/en/latest/) which is implemented as follows,

{%highlight python%}
from joblib import Parallel, delayed

N, Ns = 1e4, 1e2
sample = np.concatenate(Parallel(n_jobs=-1)(delayed(metropolis)(N=N) for _ in range(int(Ns))))
{%endhighlight%}

This gives much more symmetrical results that nonetheless explore much of the feature space.

![betterN]({{site.baseurl}}/assets/img/betterN.png)

## Another Example: The Double Gaussian

To further illustrate the issue of not exploring the full parameter space, we consider the example of a continuous random variable whose p.d.f. is defined by,

$$f(x; \mu, \sigma^2) = \frac{1}{2\sqrt{2\pi}\sigma} \left[\exp\left(\frac{(x - \mu)^2}{2\sigma^2}\right) + \exp\left(\frac{(x + \mu)^2}{2\sigma^2}\right)\right]$$

which is essentially a sum of two Gaussians centred at $$\pm \mu$$. We now run the continuous equivalent of the Metropolis-Hastings algorithm for this double Gaussian with $$\mu = 0$$ and $$\mu = 2.0$$. This is shown in the figure below, where we see that indeed in the latter case, the algorithm gets stuck on one side of the distribution, unable to jump to the other maxima.

![bad]({{site.baseurl}}/assets/img/MCMC_bad.png) ![badJoint]({{site.baseurl}}/assets/img/MCMCjoint_bad.png)

Taking the same approach as in the discrete case, we now parallelise the sampling and concatenate the result. This leads to much better performance as shown below.

![good]({{site.baseurl}}/assets/img/MCMC_good.png) ![goodJoint]({{site.baseurl}}/assets/img/MCMCjoint_good.png)


The full code for this example is given below:

{%highlight python%}
import matplotlib.pyplot as plt
import numpy as np
from joblib import Parallel, delayed
import seaborn as sns
from scipy.stats import gaussian_kde

class DoubleGaussian:
	def __init__(self, mu=0.0, sigma=1.0):
		self.mu = mu
		self.sigma = sigma
		def normal_dist(x):
			return 1/(2*np.sqrt(2*np.pi)*sigma)*np.exp(-np.power(x - mu, 2)/(2*np.power(sigma, 2))) \
				 + 1/(2*np.sqrt(2*np.pi)*sigma)*np.exp(-np.power(x + mu, 2)/(2*np.power(sigma, 2)))
		self.evaluate = normal_dist

def metropolis(dist, N):
	x = np.random.uniform(-1, 1)
	p = dist(x)
	pts = []
	for i in range(N):
		xn = x + np.random.uniform(-1, 1)
		pn = dist(xn)
		if pn >= p:
			p = pn
			x = xn
		else:
			u = np.random.rand()
			if u < pn/p:
				p = pn
				x = xn
		pts.append(x)
	pts = np.array(pts)
	return pts


if __name__ == '__main__':

    plt.figure()

    N = 500

    mu = 0.0
    sigma = 1.0

    pdf = DoubleGaussian(mu=mu, sigma=sigma)
    print('Mean:\t', pdf.mu)
    print('Sigma:\t', pdf.sigma)
    print('p(0):\t', '{:.3f}'.format(pdf.evaluate(x=0.0)))

    pts1 = np.concatenate(Parallel(n_jobs=-1)(delayed(metropolis)(dist=pdf.evaluate, N=N) \
    										  for _ in range(500)))

    # kernel = gaussian_kde(pts1)
    test = np.linspace(-5, 5, 1000)
    # plt.plot(test, kernel.evaluate(test), 
    # 	ls='--',
    # 	c='k',
    # 	lw=1.0)
    plt.plot(test, pdf.evaluate(test),
    	ls='--',
    	c='#D1495B',
    	lw=1.0,
        label=r'$\mathrm{True}\,\,\mathrm{Distr.}$')

    plt.hist(pts1, 
    	bins=20, 
    	density=True, 
    	alpha=1.0,
    	label=r'$\mu = {:.1f}, \sigma = {:.1f}$'.format(mu, sigma),
    	histtype='step'
    	)

    print('\n')

    mu = 2.0
    sigma = 0.5

    pdf = DoubleGaussian(mu=mu, sigma=sigma)
    print('Mean:\t', pdf.mu)
    print('Sigma:\t', pdf.sigma)
    print('p(0):\t', '{:.3f}'.format(pdf.evaluate(x=0.0)))

    pts2 = np.concatenate(Parallel(n_jobs=-1)(delayed(metropolis)(dist=pdf.evaluate, N=N) \
    										  for _ in range(500)))

    # kernel = gaussian_kde(pts2)
    test = np.linspace(-5, 5, 1000)
    # plt.plot(test, kernel.evaluate(test), 
    # 	ls='--',
    # 	c='k',
    # 	lw=1.0)
    plt.plot(test, pdf.evaluate(test),
    	ls='--',
    	c='#D1495B',
    	lw=1.0)

    plt.hist(pts2, 
    	bins=20, 
    	density=True, 
    	alpha=1.0, 
    	label=r'$\mu = {:.1f}, \sigma = {:.1f}$'.format(mu, sigma),
    	histtype='step'
    	)

    plt.xlabel(r'$x$')
    plt.ylabel(r'$p(x)$')
    plt.legend(fontsize=10, loc='best', title='$N = {}$'.format(N), title_fontsize=12)
    plt.xlim(-5, 5)
    plt.ylim(0, 0.6)
    #plt.title(r'MCMC: Double Gaussian')
    plt.savefig('MCMC_good.png')

    sns.jointplot(pts1, pts2,
    	kind='hex',
    	xlim=(-4,4),
    	ylim=(-4,4),
    	color='#D1495B'

    	).set_axis_labels(r'$\mathcal{N}_2(0, 1)$', r'$\mathcal{N}_2(2, 0.5)$')
    plt.savefig('MCMCjoint_good.png')
{%endhighlight%}

Bayesian Parameter Estimation
-----------------------------

THe other major use of Monte Carlo Markov Chains is in Bayesian parameter estimation. A prominent example of this is in Cosmology, where $$\Lambda\mathrm{CDM}$$ models are fitted to CMB/Galaxy survey/weak lensing data. In this case, instead of sampling from a distribution, the aim is to estimate the posterior distribution (i.e. generate a representative sample that approximately represents the underlying posterior). This process is encoded in Bayes' rule; suppose we have some data $$X = \{X_0, X_1, X_2, \cdots\}$$ and a statistical model that tells us the probabiltiy of a given realisation of the data given some set of parameters $$\theta = (\theta_1, \theta_2, \cdots)$$. Suppose further that we also have a *prior* view on the value of the parameters, denoted $$p(\theta)$$. Bayes' theorem then lets us calculate the probability that the set of parameters are the true parameters given the data, the *posterior* distribution;

$$p(\theta | X) = \frac{p(X | \theta)p(\theta)}{P(X)}, \quad p(X) = \int{\mathrm{d}\mu_\theta p(X | \theta) p(\theta)}$$

Now, the reason MCMC methods are useful in this case is that the denominator of this fraction is difficult to calculate in general especially for complex models. Since the MCMC only requires ratios of posterior probabilities however, this difficult factor cancels out in expressions such as,

$$\frac{p(\theta_i | X)}{p(\theta_j | X)} = \frac{p(X | \theta_i) p(\theta_i)}{p(X | \theta_j) p(\theta_j)}$$

which is essentially a likelihood ratio. Now we can simply follow the Metropolis-Hastings algorithm to use a Monte Carlo Markov Chain to sample from the posterior distribution. This has the benefit that we can estimate not only the best-fit parameters, but also an error in these parameters since we have access to the full distribution. We can implement the algorithm as follows,

1. Sample a random point in the parameter space $$\theta_i$$ according to the prior
2. Sample another random point in the parameter space $$\theta_j$$
3. Compute $$\ell(X, \theta_i) = p(X; \theta_i)p(\theta_i)$$ and $$\ell(X, \theta_j) = p(X; \theta_j)p(\theta_j)$$
4. If $$\ell(X, \theta_j) > \ell(X, \theta_i)$$, keep the new value, $$\theta_j$$, then repeat, else keep the old value with probabiltiy $$1 - \ell(X, \theta_j)/\ell(X, \theta_i)$$

This final step can be rewritten in terms of the log-likelihood. If we accept the new parameter values with probability $$p \sim U[0, 1]$$, then we accept the update if,

$$\log p < \sum_{i \in \mathrm{data}}{\left[\log p(x_i | \theta_j) - \log p(x_i | \theta_i)\right]} + \log p(\theta_j) - \log p(\theta_i)$$

## An Example: Fitting a Normal Distribution

We take possibly the simplest example and consider data distributed according to a standard normal distribution, $$X \sim \mathcal{N}(0, 1)$$. Then we want to find the posterior distribution on the mean, $$\mu$$, for fixed variance. The code to implement this is shown below in the case of a flat prior,

{%highlight python%}
import numpy as np
from scipy.stats import norm
import matplotlib.pyplot as plt

class BayesModel:
	def __init__(self, data, prior_range):
		self.data = data
		def pdata(data, mu):
			r"""
			Probability of :math:`x` given :math:`theta`, assuming :math:`\sigma = 1`.
			"""
			return norm.pdf(data, loc=mu).prod()
		self.pdata = pdata
		def priorprob(mu):
			r"""
			Flat prior on the mean
			"""
			return 1/(prior_range[1] - prior_range[0])
		self.priorprob = priorprob

		def samplemu(size=1):
			r"""
			Sample the parameter from the flat prior on the mean
			"""
			return np.random.uniform(prior_range[0], prior_range[1], size=size)
		self.samplemu = samplemu

def metropolis(model, N):
	mu = model.samplemu()
	posterior = []

	for _ in range(int(N)):
		new_mu = model.samplemu()
		
		old_posterior = model.pdata(data, mu)*model.priorprob(mu)
		new_posterior = model.pdata(data, new_mu)*model.priorprob(new_mu)

		paccept = new_posterior/old_posterior

		if (np.random.uniform(0, 1) < paccept):
			mu = new_mu
		posterior.append(mu)

	return np.array(posterior)

if __name__ == '__main__':
	Ndata = 100
	N = 10000

	data = np.random.normal(0.0, 1.0, Ndata)
	prior_range = [-2.0, 2.0]

	bayes = BayesModel(data, prior_range)

	posterior = metropolis(bayes, N=N)

	plt.figure(figsize=(10, 5))
	ax = plt.subplot(121)
	ax.hist(data,
		bins=max(10, int(np.sqrt(len(data)))),
		color='k',
		density=True,
		histtype='stepfilled',
		label='Data',
		alpha=0.4)
	ax.legend(fontsize=10,
		loc='upper left',
		title_fontsize=12,
		title=r'$N_{\mathrm{data}} =$' + r'${}$'.format(len(data)))
	ax.set_title('Data')
	ax.set_xlabel(r'$x$')
	ax.set_ylabel(r'$p(\mathrm{data} \sim \mathcal{N}(0, 1)$')


	ax = plt.subplot(122)
	ax.hist(bayes.samplemu(size=1000),
		bins=100,
		density=True,
		histtype='stepfilled',
		label='Prior',
		alpha=0.3)
	ax.hist(posterior, 
		bins=int(np.sqrt(len(posterior))),
		density=True,
		histtype='stepfilled',
		label='Posterior',
		alpha=0.3)
	ax.legend(fontsize=10,
		loc='upper left',
		title=r'$\hat{\mu} =$' + r'${:.2f} \pm {:.2f},$'.format(np.mean(posterior), np.std(posterior)) + '\n' + r'$N = {}$'.format(len(posterior)),
		title_fontsize=12)
	ax.set_title('Posterior Distribution')
	ax.set_xlabel(r'$\mu$')
	ax.set_ylabel(r'$p(\mu | \mathrm{data} \sim \mathcal{N}(0, 1)$')
	plt.savefig('bayes.png')
{%endhighlight%}

Running this gives the results shown below,

![flatBayes]({{site.baseurl}}/assets/img/flatbayes.png)

We could also have run with a normally distributed prior on the mean giving,

![normBayes]({{site.baseurl}}/assets/img/normbayes.png)

We see that the choice of prior makes some difference, but the resulting posterior distribution is still in the correct region.

<a href="{{site.baseurl}}/"><i class="fa fa-home" aria-hidden="true"></i> Homepage</a>